{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ddpm import config as _config\n",
    "from ddpm.config import cifar10_config\n",
    "from ddpm.data import get_cifar10_datasets\n",
    "from ddpm.diffusion_model import DiffusionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_config.DEBUG = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"System info: \")\n",
    "print(\"Device:\", device)\n",
    "print(\"Device count\", torch.cuda.device_count())\n",
    "print(\"GPU Device:\", torch.cuda.get_device_name(0))\n",
    "print(\"GPU RAM:\", f\"{(torch.cuda.get_device_properties(0).total_memory / 1e9).__round__(2)} GB\")\n",
    "\n",
    "cifar10_config.res_net_config.initial_pad = 0\n",
    "batch_size = cifar10_config.batch_size\n",
    "\n",
    "# max_epochs = 500\n",
    "max_epochs = 8\n",
    "\n",
    "each_epochs = max_epochs // torch.cuda.device_count()\n",
    "\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader, test_loader = get_cifar10_dataloaders(batch_size=batch_size)\n",
    "train_dataset, test_dataset = get_cifar10_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionModel(cifar10_config)\n",
    "\n",
    "# Create an EMA model (exact copy of the original model)\n",
    "model_ema = DiffusionModel(cifar10_config)\n",
    "model_ema.load_state_dict(model.state_dict())\n",
    "model_ema.eval()\n",
    "\n",
    "# Utility function to update EMA weights\n",
    "def update_ema(model, ema_model, alpha=0.9999):\n",
    "    \"\"\"EMA update for each parameter.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        for p, p_ema in zip(model.parameters(), ema_model.parameters()):\n",
    "            p_ema.data = alpha * p_ema.data + (1 - alpha) * p.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rank, world_size):\n",
    "    dist.init_process_group(backend='nccl', world_size=world_size, rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    rank_model = model.to(rank)\n",
    "    rank_model = nn.parallel.DistributedDataParallel(rank_model, device_ids=[rank])\n",
    "    rank_model_ema = \n",
    "\n",
    "    sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)\n",
    "    dataloader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
    "    \n",
    "    optimizer = optim.Adam(rank_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # CosineAnnealingLR will decay the LR smoothly over max_epochs\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "    \n",
    "    \n",
    "    for epoch in range(each_epochs):\n",
    "        start_time = time.time()\n",
    "        sampler.set_epoch(epoch)\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(rank)\n",
    "            labels = labels.to(rank)\n",
    "\n",
    "            loss = rank_model(images, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            update_ema(rank_model, rank_model_ema)\n",
    "        scheduler.step()\n",
    "        \n",
    "        rank_model_ema.eval()\n",
    "        end_time = time.time()\n",
    "        print(f\"Rank {rank} Epoch {epoch} took {end_time - start_time:.3f} seconds\")\n",
    "\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "mp.spawn(train, args=(world_size,), nprocs=world_size)\n",
    "\n",
    "time.time() - start_time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avocado",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
